{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "import itertools\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import bottleneck as bn\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import networkx.algorithms.approximation as nxa\n",
    "import igraph as ig\n",
    "# http://stackoverflow.com/questions/35279733/what-could-cause-networkx-pygraphviz-to-work-fine-alone-but-not-together\n",
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "from sklearn import decomposition\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".95\"})\n",
    "%matplotlib inline\n",
    "\n",
    "import missingno as msno\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "from speclib.loaders import (loadUserPhonenumberDict, getUserList, Useralias,\n",
    "                             loadUserParallel, dict2DataFrame, users2DataFrame)\n",
    "from speclib.plotting import looseAxesLimits, barSBS, countsOnBarPlot, plotNeatoGraph, nxQuickDraw\n",
    "from speclib.graph import networkx2igraph, igraph2networkx, userDF2nxGraph, userDF2activityDataframe\n",
    "from speclib.misc import nanEqual, timedelta2unit, standardizeData\n",
    "\n",
    "\n",
    "for k, v in {'font.size': 13.0,\n",
    "             'legend.fontsize': 13.0,\n",
    "             'axes.labelsize': 12.0,\n",
    "             'axes.titlesize': 15.0,\n",
    "             'figure.figsize': [16.0, 7.0],\n",
    "             'figure.dpi': 200,\n",
    "             'figure.titlesize': 'large',\n",
    "             'xtick.labelsize': 13.0,\n",
    "             'ytick.labelsize': 13.0}.items():\n",
    "    mpl.rcParams[k] = v\n",
    "\n",
    "%load_ext watermark\n",
    "%watermark -a \"Allan Leander Rostock Hansen\" -u -d -v -p numpy,bottleneck,pandas,matplotlib,sklearn,missingno,networkx,igraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Cleaning the Data\n",
    "\n",
    "## Loading the Data\n",
    "\n",
    "Load user SMS and call data.\n",
    "Load binary datastrucctures stored on disk if they exist and otherwise load the data from the original files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if os.path.isfile('phone_df.h5'):\n",
    "    df = pd.io.pytables.read_hdf('phone_df.h5', 'df')\n",
    "    import pickle\n",
    "    with open('useralias.pk', 'br') as fid:\n",
    "        ua = pickle.load(fid)\n",
    "else:\n",
    "    ua = Useralias()\n",
    "    userSpec = [(user, ua[user], ('sms', 'call')) for user in getUserList()]\n",
    "    userData = loadUserParallel(userSpec) \n",
    "    df = users2DataFrame(userData, ua)\n",
    "    del userData\n",
    "phonebook = loadUserPhonenumberDict(ua) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning data\n",
    "\n",
    "Make a subset of the data only containing communications contained within the Social Frabric project. Of this data, select a subset of the data containing the most active users, preferebly who communicate with each other… a clique percolation algorithm could be used for this, but that won't be the initial approach.\n",
    "\n",
    "A measure of the activity could simply be\n",
    "$$ a = \\sum_{\\text{i}}\\frac{\\mathrm{user_{sms}}_i}{\\sum_i \\mathrm{user_{sms}}_i} + \\frac{\\mathrm{user_{call}}_i}{\\sum_i \\mathrm{user_{call}}_i} $$\n",
    "\n",
    "but this could yield a huge $a$ for a very active, yet weakly connected user, so a weighting with the number of contacted people shoud be introduced.\n",
    "\n",
    "Another approach is to naively sum up all events between users.\n",
    "\n",
    "Since a conversation using SMS regesters as several events for both users (usually), whereas a conversation carried out over a call registes as one event, a weighting should be introduced.\n",
    "The easy solution is to divide the adjacency matrices with the sum of all the entries, meaning that the sum of all the elements would both add up to one.\n",
    "Yet another approach would be to clean the SMS data in the following way:\n",
    "\n",
    "1. Investigate the distribution of time between a SMS and a reply to it.\n",
    "2. Use the distribution to determining a typical reply time.\n",
    "3. Remove entries in the SMS data which weren't replied to within some number, say 3, times the average reply time.\n",
    "\n",
    "Cleaning the SMS data as proposed above, should also prompt for a similar cleaning of the call data.\n",
    "An obvious way would be to remove unansvered calls, albeit the SMS dataset should also be checked for an \"answer\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing the data munging\n",
    "\n",
    "Remove rows for which the contacted number is not present in `phonebook` (userhash to phonehash translation table).\n",
    "\n",
    "Also add a column which contaings the useralias (`u0001`, `u0345` and so on) for the contacted user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df[df.number.apply(lambda num: num in phonebook)] \n",
    "df['contactedUser'] = df.number.apply(lambda x: phonebook[x]) \n",
    "df.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count number of unique contacts for each user.\n",
    "This is what's called the degree distribution in graph theory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "userUniqueCommCall = dict()   # dict for calls\n",
    "userUniqueCommSms = dict()  # dict for sms\n",
    "for user in df.index.get_level_values('user').unique():  # loop over users\n",
    "    try:  # 'call' data might be missing from some users\n",
    "        comSer = df.loc[user, 'call'].contactedUser\n",
    "        userUniqueCommCall[user] = comSer.unique().size\n",
    "    except KeyError:\n",
    "        userUniqueCommCall[user] = 0\n",
    "    try:  # 'sms' data might be missing from some users\n",
    "        comSer = df.loc[user, 'sms'].contactedUser\n",
    "        userUniqueCommSms[user] = comSer.unique().size\n",
    "    except KeyError:\n",
    "        userUniqueCommSms[user] = 0\n",
    "\n",
    "userUniqueComm = pd.DataFrame(pd.Series(userUniqueCommCall), columns=('call',))\n",
    "userUniqueComm['sms'] = pd.Series(userUniqueCommSms)\n",
    "userUniqueComm['total'] = userUniqueComm.sms + userUniqueComm.call\n",
    "del userUniqueCommCall\n",
    "del userUniqueCommSms\n",
    "\n",
    "userUniqueComm.sort(columns='total', inplace=True, ascending=False)\n",
    "display(userUniqueComm.head(), userUniqueComm.describe()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the findings, using two different plot styles, using both normal and log scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 8))\n",
    "ax0 = (plt.subplot2grid((12, 2), (0, 0), rowspan=4))\n",
    "ax1 = (plt.subplot2grid((12, 2), (0, 1), rowspan=4))\n",
    "ax2 = (plt.subplot2grid((12, 2), (6, 0), colspan=4, rowspan=9))\n",
    "\n",
    "# Normal scaling\n",
    "# userUniqueComm.plot.line(ax=ax0) \n",
    "# userUniqueComm.total.value_counts().plot.hist(bins=(userUniqueComm.total.max()), ax=ax0) \n",
    "userUniqueComm.drop('total', axis=1).plot.hist(bins=userUniqueComm.drop('total', axis=1).max().max(),\n",
    "                                               stacked=True,\n",
    "                                               ax=ax0)\n",
    "ax0.set_xlabel('Connectivity degree')\n",
    "userUniqueComm.drop('total', axis=1).plot.area(ax=ax1)\n",
    "ax1.set_xlabel('Users'), ax1.set_ylabel('Connectivity')\n",
    "fig.suptitle('Unique users conacted using Calls and SMS')\n",
    "\n",
    "# Log scaling\n",
    "userUniqueComm.plot.line(ax=ax2) \n",
    "ax2.set_yscale('log')\n",
    "ax2.grid(which='minor') \n",
    "ax2.set_xlabel('Users'), ax2.set_ylabel('Connectivity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A threshold of 20 unique contacts is used to select the most active users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "totalCommunicationThreshold = 20\n",
    "chosenUsers = userUniqueComm.index[userUniqueComm.total > totalCommunicationThreshold]\n",
    "print(*chosenUsers, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute adjacency matrices\n",
    "Construct DataFrames for adjacency matrices/graphs for call and SMS data, where the index is the user initiating contact, and the columns is the users targeted by said contact.\n",
    "Selected users is limited to previously chosen active users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adCall = dict()  # dict for user calls\n",
    "adSms = dict()  # dict for user sms\n",
    "\n",
    "# for user in df.index.get_level_values('user').unique():\n",
    "for user in chosenUsers:\n",
    "    if 'call' in df.loc[user].index:  # user might not have 'call' data\n",
    "        cnt = df.loc[user, 'call'].contactedUser.value_counts()\n",
    "        adCall[user] = cnt.to_dict()\n",
    "    if 'sms' in df.loc[user].index:  # user might not have 'sms' data\n",
    "        cnt = df.loc[user, 'sms'].contactedUser.value_counts()\n",
    "        adSms[user] = cnt.to_dict() \n",
    "\n",
    "# Convert dicts to DataFrames and label the index and columns.\n",
    "adCall = pd.DataFrame(adCall)\n",
    "adSms = pd.DataFrame(adSms)\n",
    "adCall.columns.name = 'userRec'\n",
    "adSms.columns.name = 'userRec'\n",
    "adCall.index.name = 'userInit'\n",
    "adSms.index.name = 'userInit'\n",
    "\n",
    "# Drop contacted users which are'nt preset in the index (contact initating users) \n",
    "adCall.drop(list(set(adCall.columns) - set(adCall.index)), axis=1, inplace=True) \n",
    "adSms.drop(list(set(adSms.columns) - set(adSms.index)), axis=1, inplace=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a column where the activity level for each user is summed up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adCall['activity'] = adCall.sum(axis=0, skipna=True)\n",
    "adSms['activity'] = adSms.sum(axis=0, skipna=True)\n",
    "\n",
    "# Sort the columns so that the 'activity' column is a the start of the Data Frame\n",
    "adCall.columns = adCall.columns.sort_values()\n",
    "adSms.columns = adSms.columns.sort_values() \n",
    "\n",
    "display(adCall.head(), adSms.head()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct graph objects\n",
    "A NetworkX graph is constructed from the DataFrame with the adjacency-matrix like data.\n",
    "The call and sms data is combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adf = adCall + adSms\n",
    "adf.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dct = dict()  # keys is users which initiate contact (adf.index)\n",
    "# Remove newthe recently added activity column and make the activity measure binary (int8 for display putposes)\n",
    "adfNoActivity = (adf[adf.columns[adf.columns != 'activity']] > 0).astype(np.int8)\n",
    "display(adfNoActivity.head())\n",
    "\n",
    "for i, iUsr in enumerate(sorted(adf.index.unique())):  # Loop througth sorted user list\n",
    "    comSeries = adfNoActivity.loc[iUsr]  # Extract user communications\n",
    "    comSeries = comSeries.index[comSeries.astype(bool)]  # Filter the usernames (index) using the series masking data\n",
    "    dct[iUsr] = comSeries.tolist()    # Convert Pandas Series to a list\n",
    "g = nx.from_dict_of_lists(dct)  # costruct graph\n",
    "\n",
    "# Delete temporary variables\n",
    "del dct\n",
    "del adfNoActivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nxQuickDraw(g) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly some nodes aren't connected to the network – their contacts probably didn't meet the \"choose any users with 20 or more individual contacts\"-criterion.\n",
    "\n",
    "Nodes with no connections (that is, nodes with degree 0) are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for node, degree in dict(g.degree()).items():\n",
    "    if degree == 0:\n",
    "        g.remove_node(node) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify by plotting the network again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nxQuickDraw(g) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indeed looks like nodes with degree 0 are removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply clique algorithms to find most active users\n",
    "\n",
    "<!--\n",
    "* Two algorithms is used.\n",
    "* I use the users returned from the biggest groups from both (14 users).\n",
    "* I also investigate the number overlap inbetween the two algorithms wrt. cliques and users.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify results from NetworkX using the iGraph library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ig = networkx2igraph(g) \n",
    "\n",
    "igraphCliques = ig.maximal_cliques() \n",
    "igraphCounter = Counter((len(el) for el in igraphCliques)) \n",
    "networkxCounter = Counter(len(el) for el in nx.algorithms.find_cliques(g))\n",
    "if networkxCounter == igraphCounter:\n",
    "    display(Markdown('Igraph and Networkx yields identical results.'))\n",
    "else:\n",
    "    display(Markdown('Igraph and Networkx yields different results!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using NetworkX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clqdf = pd.DataFrame(nx.algorithms.find_cliques(g))\n",
    "clqdf['cliquesize'] = (~clqdf.isnull()).sum(axis=1)\n",
    "clqdf.sort(columns='cliquesize', ascending=False, inplace=True) \n",
    "clqdf.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate the cliques\n",
    "\n",
    "Plot clique size distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "clqdf.cliquesize.value_counts().plot.bar(ax=ax, rot=0)\n",
    "ax.set_title(\"Clique size distribution\")\n",
    "looseAxesLimits(ax, [0.0, 0.0, 0.0, 0.1])\n",
    "countsOnBarPlot(ax)\n",
    "ax.set_xlabel('Clique size')\n",
    "ax.set_ylabel('Number of cliques') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the average number of connections vs. clique size?\n",
    "The DataFrame containing the cluques are as follows.\n",
    "\n",
    "Calculating the mean connectivity and standard deviation for each clique…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lst = list()\n",
    "for i in range(clqdf.shape[0]):\n",
    "    nodeArr, cliqueSize = clqdf.iloc[i][:-1], clqdf.iloc[i][-1]\n",
    "    cliqueConnLst = [el[1] for el in nx.degree(g, nodeArr)]\n",
    "    lst.append((cliqueSize, np.mean(cliqueConnLst), np.std(cliqueConnLst)))\n",
    "\n",
    "clqStatDf = pd.DataFrame(lst, columns=['cliqueSize', 'cliqueMean', 'cliqueStd'])\n",
    "del lst\n",
    "clqStatDf.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "… and takeing the mean for each group size…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clqStatDfMean = clqStatDf.groupby('cliqueSize').mean()\n",
    "clqStatDfMean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "…which can be visualized as a violin plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Violinplot\n",
    "fig, ax = plt.subplots()\n",
    "sns.violinplot(x='cliqueSize', y='cliqueMean', data=clqStatDf, split=True, scale='count', inner='stick', color='0.8', ax=ax) \n",
    "\n",
    "# Errorbars\n",
    "(_, caps, _) = ax.errorbar(range(clqStatDfMean.shape[0]), clqStatDfMean.cliqueMean, clqStatDfMean.cliqueStd,\n",
    "                           fmt='o', color='orange', elinewidth=2, alpha=0.75, barsabove=True,\n",
    "                           capsize=5, label=r'$\\mu$ and $\\sigma$')\n",
    "# Caps on error bars\n",
    "for cap in caps:\n",
    "    cap.set_markeredgewidth(2)\n",
    "    \n",
    "ax.legend(loc='best')\n",
    "\n",
    "# Take 2\n",
    "fig, ax = plt.subplots()\n",
    "sns.violinplot(x=\"cliqueSize\", y=\"cliqueMean\", data=clqStatDf, inner=None, color=\"0.8\", scale='count', ax=ax)\n",
    "\n",
    "sns.stripplot(x=\"cliqueSize\", y=\"cliqueMean\", data=clqStatDf, jitter=0.04, ax=ax, color='k', size=3, alpha=0.5)\n",
    "\n",
    "# Errorbars\n",
    "(_, caps, _) = ax.errorbar(range(clqStatDfMean.shape[0]), clqStatDfMean.cliqueMean, clqStatDfMean.cliqueStd,\n",
    "                           fmt='o', color='orange', elinewidth=2, alpha=0.75, barsabove=True, capsize=5,\n",
    "                           label=r'$\\mu$ and $\\sigma$')\n",
    "# Put end caps on the error bars\n",
    "for cap in caps:\n",
    "    cap.set_markeredgewidth(2)\n",
    "\n",
    "ax.legend(loc='best') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plotting a histogram of the clique mean accross all clique sizes, clearly shows a bimodal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax0 = plt.subplots()\n",
    "ax1 = ax0.twinx()\n",
    "histGridColor = np.array((242, 182, 138))/255\n",
    "histPlotColor = np.array((237, 153, 92))/255\n",
    "densGridColor = np.array(((205, 128, 146)))/255\n",
    "densPlotColor = np.array((184, 73, 99))/255\n",
    "\n",
    "clqStatDf.cliqueMean.plot.hist(bins=75, ax=ax0, color=histPlotColor, edgecolor='w')\n",
    "clqStatDf.cliqueMean.plot.density(ax=ax1, color=densPlotColor)\n",
    "ax0.grid(axis='y', color=histGridColor)\n",
    "ax1.grid(axis='y', color=densGridColor)\n",
    "ax0.tick_params(axis='y', colors=histPlotColor)\n",
    "ax1.tick_params(axis='y', colors=densPlotColor)\n",
    "ax0.yaxis.label.set_color(histPlotColor)\n",
    "ax1.yaxis.label.set_color(densPlotColor)\n",
    "ax0.set_xlabel('Connectivity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose users from the largest clique: 6\n",
    "Remove users which are not in a clique with size 6 or larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clqdf = clqdf[clqdf.cliquesize >= 6]\n",
    "clqdf.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Make an array containing all the users in the selected clique(s) only once.\n",
    "* From that array, generate a new array which includes all the contacts of those users.\n",
    "* Extract a subgraph for those users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coreUsers = pd.Series(clqdf.drop('cliquesize', axis=1).values.flat).dropna().unique()  # Unique list of chosen users\n",
    "remoteUsers = [tuple(nx.neighbors(g, user)) for user in coreUsers]  # Chosen users neighbours\n",
    "remoteUsers = pd.Series(pd.DataFrame(remoteUsers).values.flat).dropna().unique()  # Make the array elements unique\n",
    "\n",
    "# Print the choice and the lengths of the array\n",
    "print('Core users in network ({} users):'.format(coreUsers.size))\n",
    "print(*coreUsers, sep='\\t', end='\\n'*2)\n",
    "print('Core users and their connections ({} users):'.format(remoteUsers.size))\n",
    "print(*remoteUsers, sep='\\t', end='\\n'*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure that the core users are included in the remote users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if set(coreUsers).issubset(set(remoteUsers)):\n",
    "    display(Markdown('All core users are contained in remote users.'))\n",
    "else:\n",
    "    display(Markdown('Remember to combine core users and remote users for subgraph extraction!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is expected, since the core users is a clique, and thus will be included among the links from the other users in the clique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the subgraph, and verify it by plotting it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gs = g.subgraph(remoteUsers)  # The subset of the graph on which a PCA analysis should be performed on the users\n",
    "\n",
    "nx.draw(gs, with_labels=True, node_color='lightblue', edge_color='lightgray', node_size=150) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gsDegdf = pd.DataFrame(gs.degree())\n",
    "gsDegdf.columns = ['user', 'degree']\n",
    "gsDegdf.set_index('user', inplace=True)\n",
    "gsDegdf.sort(columns='degree', ascending=False, inplace=True)\n",
    "display(gsDegdf.head())\n",
    "\n",
    "fig, ax0 = plt.subplots()\n",
    "ax0.hist(gsDegdf.degree,\n",
    "        range=(gsDegdf.degree.min(), gsDegdf.degree.max()+1),\n",
    "        bins=gsDegdf.degree.max()+1 - gsDegdf.degree.min(),\n",
    "        edgecolor='white')\n",
    "ax0.minorticks_on() \n",
    "ax0.grid(True, which='both')\n",
    "ax0.set_xticks(range(0, gsDegdf.degree.max()+2, 5))\n",
    "ax0.set_xbound(0, gsDegdf.degree.max()+2)\n",
    "ax0.set_xlabel('Connectivity degree')\n",
    "ax0.set_ylabel('Number of users')\n",
    "# countsOnBarPlot(ax0)\n",
    "ax0.set_title('Conectivity vs number of users')\n",
    "\n",
    "# ax1 = ax0.twinx()\n",
    "# gsDegdf.plot.density(ax=ax1, color='orange')\n",
    "# ax1.grid(b=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do the PCA analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the adjacency matrix for the chosen network and plot it.\n",
    "Clearly one user is _very_ connected.\n",
    "Consider removing said user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gsa = nx.adj_matrix(gs) \n",
    "fig, ax = plt.subplots(figsize=(4, 4), dpi=150) \n",
    "ax.pcolorfast(gsa.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp = max(gs.degree(), key=lambda nd: nd[1]) \n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gs.remove_node(tmp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gsa = nx.adj_matrix(gs) \n",
    "fig, ax = plt.subplots(figsize=(4, 4), dpi=150) \n",
    "ax.pcolorfast(gsa.todense()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Binning activities\n",
    "### Do the time binning of activities\n",
    "Now the chosen users' activities are about to be time binned.\n",
    "Bins will initially be binned in sig hour bins, initially starting from midnight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfa = df.loc[remoteUsers.tolist()]\n",
    "dfa.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's some times in the timebin which needs to be removed, since the project didn't exist in 1970!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfa.timestamp.describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfa['year'] = dfa.timestamp.dt.year\n",
    "dfa.year.value_counts().plot.bar() \n",
    "countsOnBarPlot(plt.gca())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the 2011 and 2012 data points. There's so few that they seem suspicious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfa[(dfa.year == 2011) | (dfa.year == 2012)].reset_index().user.unique() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Indeed there's only one user responsible for these points, so it's likely that the uses phone just had some wrong date settings, and the data will be removed for this purpose, and only data from 2015 and onwards will be considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfa = dfa[dfa.year >= 2013]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find minimum timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfa.timestamp.min() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the corresponding unix time integer, use modolu operator for 6-hour intervans, substract it from the integer time, and cast back to a Timestamp.\n",
    "This is the starting point for the time binning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "binWidth = 3600*6  # 6 hours in seconds\n",
    "startTime = dfa.timeint.min() - (dfa.timeint.min() % binWidth)\n",
    "display(pd.Timestamp(startTime, unit='s'), dfa.timestamp.min() - pd.Timestamp(startTime, unit='s') )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the timebins using integer division and examine the timebins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfa['timebin'] = (dfa.timeint - startTime) // binWidth\n",
    "\n",
    "display(dfa.timebin.describe()) \n",
    "\n",
    "fig, ax0 = plt.subplots()\n",
    "hy, hx = np.histogram(dfa.timebin, bins=400)\n",
    "hx = hx[1:]\n",
    "ax0.bar(hx, hy, width=10, label='Timebins')\n",
    "ax1 = ax0.twinx()\n",
    "hy = np.convolve(np.ones(8), hy, 'same')\n",
    "ax0._get_lines.get_next_color() \n",
    "ax1.plot(hx, hy, color=ax0._get_lines.get_next_color(), label='Timebins (smoothed)') \n",
    "ax0.set_xlabel('Timebins')\n",
    "ax0.set_ylabel('Timebins')\n",
    "ax1.set_ylabel('Timebins (smoothed)') \n",
    "ax0.grid(axis='x')\n",
    "ax1.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also visualize using a density kernel plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfa.timebin.plot.density() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfts = pd.DataFrame(index=remoteUsers, columns=np.arange(dfa.timebin.min(), dfa.timebin.max()+1))\n",
    "\n",
    "for user in remoteUsers:\n",
    "    dfts.loc[user] = dfa.loc[user].timebin.value_counts()\n",
    "\n",
    "display(dfts.head())\n",
    "\n",
    "dfts.replace(np.NaN, 0.0, inplace=True) \n",
    "fig, (ax0, ax1) = plt.subplots(1, 2)\n",
    "ax0.grid(axis='y')\n",
    "ax0.set_facecolor('black')\n",
    "ax0.pcolorfast(np.log10(dfts.values), cmap=mpl.cm.binary_r)\n",
    "ax0.set_xlabel('Timebins')\n",
    "ax0.set_ylabel('Users')\n",
    "ax0.set_yticklabels([])\n",
    "ax0.set_title(r'$\\log_{10}$ user activity')\n",
    "ax1.grid(axis='y')\n",
    "ax1.set_facecolor('black')\n",
    "ax1.pcolorfast(np.log10(np.cumsum(dfts.values, axis=1)), cmap=mpl.cm.binary_r)\n",
    "ax1.set_xlabel('Timebins')\n",
    "ax1.set_yticklabels([])\n",
    "ax1.set_ylabel('Users')\n",
    "ax1.set_title(r'$\\sum \\log_{10}$ user activity')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the binned data to a format which is useable in with the PCA algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing that the reshaping does the right thing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mat2PcaVec = lambda x: x.reshape((1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp = np.arange(12)[:,np.newaxis].reshape((3, -1))\n",
    "display(tmp, mat2PcaVec(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "toPca = dfts.values\n",
    "toPcaMean = toPca.mean(axis=0)\n",
    "toPcaStd = toPca.std(axis=0)\n",
    "toPcaStd[toPcaStd == 0] = 1.0\n",
    "toPca = (toPca - toPcaMean)/toPcaStd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, (ax0, ax1) = plt.subplots(1, 2)\n",
    "ax0.bar(*np.unique(np.sign(toPca.flat), return_counts=True))\n",
    "ax0.set_xticks([-1, 0, 1])\n",
    "ax0.set_xlabel('Sign of data entry')\n",
    "ax0.set_ylabel('Count')\n",
    "\n",
    "ax1.hist(toPca.flat, 100)\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_yscale('log')\n",
    "ax1.grid(which='minor')\n",
    "ax1.set_xlabel('Value in adjacency matrix')\n",
    "ax1.set_ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = decomposition.PCA()\n",
    "pca.fit(toPca)\n",
    "explVarRat = pca.explained_variance_ratio_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(explVarRat, 'o-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Experiment with other grouping/binning options\n",
    "\n",
    "Combine activity for all hours on a weekly basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfWH = dfa.drop(['timebin', 'year', 'timeint', 'weekday', 'hour'], axis=1)  # HW is short for Weekday Hour\n",
    "# dfWH.reset_index(inplace=True)\n",
    "# dfWH.set_index('timestamp', inplace=True)\n",
    "dfWH.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp = dfWH.groupby((dfWH.index.get_level_values(0), dfWH.timestamp.dt.weekday, dfWH.timestamp.dt.hour))\n",
    "dfWHCnt = tmp.count().drop(['body', 'duration', 'number', 'timestamp'], axis=1)\n",
    "dfWHCnt.rename(index=None, columns={'contactedUser': 'events'}, inplace=True) \n",
    "dfWHCnt.index.rename(['user', 'weekday', 'hour'], inplace=True)\n",
    "dfWHCnt.head(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "whCntMat = np.zeros((remoteUsers.size, 7*24))  # 7 days a week, 24 hours a day\n",
    "\n",
    "user2row = {remoteUsers[i]: i for i in range(len(remoteUsers))}\n",
    "user2col = lambda weekday, hour: 24*weekday + hour\n",
    "for (user, weekday, hour) in dfWHCnt.index:\n",
    "    r = user2row[user]\n",
    "    c = user2col(weekday, hour)\n",
    "    whCntMat[r,c] = dfWHCnt.loc[user, weekday, hour]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.pcolorfast(whCntMat) \n",
    "ax.set_xticks(np.arange(12, whCntMat.shape[1]-5, 12))\n",
    "# ax.set_xticklabels([])\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xlabel('Bins')\n",
    "ax.set_ylabel('Users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = decomposition.PCA()\n",
    "pca.fit(whCntMat)\n",
    "evrFloat = pca.explained_variance_ratio_\n",
    "pca.fit(whCntMat > 0)\n",
    "evrBool = pca.explained_variance_ratio_\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(evrFloat[:-1], '-o', label='float')  # Last point omitted, practically 0\n",
    "ax.plot(evrBool[:-1], '-o', label='bool')  # Last point omitted, practically 0\n",
    "ax.set_xlabel('Vectors')\n",
    "ax.set_ylabel('Explanied variance ratio')\n",
    "ax.set_yscale('log') \n",
    "ax.grid(which='minor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(evrFloat, '-o', label='float')  \n",
    "ax.plot(evrBool, '-o', label='bool')  \n",
    "ax.set_xlabel('Vectors')\n",
    "ax.set_ylabel('Explanied variance ratio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.isnan(pca.get_covariance()).any() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(facecolor='white')\n",
    "pc = ax.pcolorfast(pca.get_covariance()) \n",
    "fig.colorbar(pc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Build adjacency matrices for each week\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nTimebins = dfa.timebin.max()\n",
    "toPcaMat = np.zeros((len(remoteUsers)**2, nTimebins))\n",
    "for tb in range(0, nTimebins):\n",
    "    weekGraph = userDF2nxGraph(dfa[dfa.timebin == tb])\n",
    "    weekGraph.remove_nodes_from(set(weekGraph.nodes()) - set(remoteUsers))  # remove users not in remoteUsers\n",
    "    weekGraph.add_nodes_from(remoteUsers)  # add users from remote users\n",
    "    toPcaMat[:, tb] = nx.adj_matrix(weekGraph, nodelist=remoteUsers).todense().T.reshape(-1)\n",
    "\n",
    "toPcaMat = standardizeData(toPcaMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "toPcaMat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = decomposition.PCA()\n",
    "pca.fit(toPcaMat) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax0 = plt.subplots()\n",
    "ax1 = ax0.twinx()\n",
    "n = 300\n",
    "l0 = ax0.plot(np.arange(n), np.cumsum(pca.explained_variance_ratio_[:n]),\n",
    "        label=r'$\\frac{\\sum_{i = 1}^{300} e_i} {\\sum_{i = 1}^{N} e_i}$', color='blue')\n",
    "l1 = ax1.plot(np.arange(n), pca.explained_variance_ratio_[:n],\n",
    "        label=r'$\\frac{e_i}{\\sum_{i=1}^{N} e_i}$', color='green')\n",
    "ax0.legend([l0[0], l1[0]], [l0[0].get_label(), l1[0].get_label()], fontsize='xx-large', loc='right')\n",
    "ax0.grid(which='minor')\n",
    "ax0.set_xlabel('Eigenvalue #') \n",
    "ax0.tick_params(axis='y', colors='blue')\n",
    "ax1.tick_params(axis='y', colors='green')\n",
    "ax0.yaxis.label.set_color('blue')\n",
    "ax1.yaxis.label.set_color('green')\n",
    "ax0.grid(axis='y', color='blue', alpha=0.4)\n",
    "ax1.grid(axis='y', color='green', alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:mpl2]",
   "language": "python",
   "name": "conda-env-mpl2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
